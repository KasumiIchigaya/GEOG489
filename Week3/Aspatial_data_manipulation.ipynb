{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<style>\n",
    "td {\n",
    "  font-size: 15px\n",
    "}\n",
    "th{\n",
    "  font-size: 15px  \n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9a48c",
   "metadata": {},
   "source": [
    "# Aspatial data manipulation: Numpy and Pandas\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda8ea19",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "\n",
    "NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more.\n",
    "<br><br>\n",
    "\n",
    "You can import the numpy library as shown below:\n",
    "```python\n",
    "import numpy as np\n",
    "```\n",
    "\n",
    "source: https://numpy.org/doc/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333887f9",
   "metadata": {},
   "source": [
    "The core of the `numpy` package is the `array` class. Let's examine that first. We can make an array out of a sequence, like a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e715998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [1, 2, 3, 4, 5, 6]\n",
    "np.array(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1b2d4",
   "metadata": {},
   "source": [
    "Note that arrays must be \"homogeneous\", unlike `list`, in that the data types of each element must be the same. The data type of the array is upcast to be able to represent all of the data. So, if only one element is a float, all elements will be converted to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fde07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = [1, 2, 3.1415, 4, 5, 6]\n",
    "np.array(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749582f6",
   "metadata": {},
   "source": [
    "Arrays are like multidimensional sequences. We can create a 2D array by supplying a list of lists as the argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf970c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([[1., 2., 3.,], [4., 5., 6.]])\n",
    "print(arr.shape)\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b9490",
   "metadata": {},
   "source": [
    "You can set the `array.shape` attribute to change the shape of the array. This attribute does not change the elements of the array, or how it is stored in memory, just how it is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b4f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr.shape = (3, 2)\n",
    "print(arr.shape)\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841a856c",
   "metadata": {},
   "source": [
    "---\n",
    "### *Exercise*\n",
    "\n",
    "1. Create an array, named `arr1`, that has value from 1 through 9. \n",
    "2. Assign `arr1` to `arr2` and resize the array into 3 by 3 (3 rows and 3 columns). \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a153516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef1e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test code for the previous function. This cell should NOT give any errors when it is run.\"\"\"\n",
    "\n",
    "# Check your result here. \n",
    "assert np.array_equiv(arr1, np.array(list(range(1, 10))))\n",
    "assert arr2.shape == (3,3)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fe0cb6",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "`pandas` is a Python package providing fast, flexible, and expressive data structures designed to make working with “relational” or “labeled” data both easy and intuitive. It is built on top of `NumPy` and is intended to integrate well within a scientific computing environment with many other 3rd party libraries.\n",
    "<br><br>\n",
    "You can import the `pandas` library as shown below:\n",
    "```python\n",
    "import pandas as pd\n",
    "```\n",
    "### Data Structure\n",
    "\n",
    "| Diemensions | Name | Description | Usage |\n",
    "| :-: | :-: | :-: | :-: |\n",
    "| 1D | Series | 1D labeled homogeneously-typed array | pd.Series() |\n",
    "| 2D | DataFrame | General 2D labeled, size-mutable tabular structure <br> with potentially heterogeneously-typed column | pd.DataFrame() |\n",
    "\n",
    "\n",
    "source: https://pandas.pydata.org/docs/getting_started/overview.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c724aecb",
   "metadata": {},
   "source": [
    "### Difference between Numpy and Pandas\n",
    "| Comparision | Numpy | Pandas | \n",
    "| :-: | :-: | :-: |\n",
    "| Input data | Preferable for Numerical data | Prefereable for Tabular data |\n",
    "| Objects | Array | Data frame and Series |\n",
    "| Efficiency | Memory efficient | Comsume more memory |\n",
    "| Data size | Better performance for 50 K or less | Better performance for 500 K or more rows |\n",
    "| Indexing | Very fast indexing | Relatively slow indexing |\n",
    "| Data demension | Can represent up to 2D with DataFrame <br> Panel used to handle 3D data but deprecated in 0.24.0 | Multi-dimension data presentation <br> with multi-dimensional arrays (ndarray) |\n",
    "\n",
    "source: https://www.geeksforgeeks.org/difference-between-pandas-vs-numpy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ebb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.__version__ # Check your pandas version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33995c5",
   "metadata": {},
   "source": [
    "## Mockup Analysis\n",
    "\n",
    "Here is the narrative of our mockup analysis. \n",
    "\n",
    "### Problem statement:\n",
    "Due to the lack and shortage of healthcare resources, many COVID-19 patients could not receive timely and adequate treatments, resulting in more fatality from the disease. \n",
    "\n",
    "### Goal of the analysis:\n",
    "We as researchers suppose that insufficient healthcare availability causes more fatality from COVID-19, so we are investigating the relationship between the availability of insensitive care unit (ICU) beds and the case-fatality ratio of COVID-19. \n",
    "\n",
    "### Data employed: \n",
    "* (Daily collected) confirmed case and fatality of COVID-19 per county\n",
    "* (Daily collected) available ICU beds count per Trauma Service Area (TSA*)\n",
    "* Lookup table for counties associated with each TSA\n",
    "\n",
    "\\* TSAs are the regions aggregating 254 counties in Texas into 22 Regional Advisory Council that sets the guidelines for the trauma care system. <br>\n",
    "\n",
    "![TSA Map in Texas](https://www.dshs.texas.gov/assets/0/76/110/783/8589936060/8589938383/554b3559-ab9d-4461-82e9-61b9ef17b4a4.jpg)\n",
    "\n",
    "Data source: https://dshs.texas.gov/coronavirus/AdditionalData.aspx\n",
    "\n",
    "### Steps\n",
    "* Import and check the input data \n",
    "* Aggregate county-level data (daily confirmed cases and death) into Trauma Service Area (TSA) for the future comparison\n",
    "* Compute 7-days moving mean\n",
    "* Conduct a correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec0308",
   "metadata": {},
   "source": [
    "## 1. Importing data\n",
    "You can import existing dataset with the various formats, such as excel, csv, tsv, or json, but not limited to. <br> Check out https://pandas.pydata.org/docs/reference/io.html. <br>\n",
    "\n",
    "#### Covered functions\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html> pd.read_excel() </a> <br>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html> pd.read_csv() </a><br>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.read_json.html> pd.read_json() </a><br>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html?> df.astype() </a><br>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html?> df.head() </a><br>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.set_index.html?> df.set_index() </a><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ca0d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_death = pd.read_excel('./data/daily_death.xlsx') # read an excel file\n",
    "daily_death = daily_death.set_index('County') # specify the index of this table so that we can call the value easily in the later computational process\n",
    "daily_death = daily_death.astype(float) # change the data type of cells as float (from integer)\n",
    "daily_death # this will call the instance of table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3835fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_case = pd.read_csv('./data/daily_case.csv') # read a csv file\n",
    "daily_case = daily_case.set_index('County')\n",
    "daily_case.head(10) # df.head() function will call the top(n) rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "icu_empty = pd.read_csv('./data/icu_empty.tsv', sep='\\t')  # we can also load tsv (tap seperated value) with read_csv() function\n",
    "icu_empty = icu_empty.set_index('TSA')\n",
    "icu_empty  # in case the table is not long enough, it will show the entire table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab504ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsa_county = pd.read_json('./data/tsa_county.json') # load a json file\n",
    "tsa_county.head() # df.head() function calls five rows as a default value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34abfbcc",
   "metadata": {},
   "source": [
    "## 2. Aggregate county-level data into Trauma Service Area (TSA) \n",
    "Given that the COVID-19 related data is collected based on the county level and the ICU availability data is based on the TSA level, we need to match their spatial resolution. \n",
    "\n",
    "#### covered functions\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html> pd.DataFrame() </a> <br>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.unique.html?> df.unique()</a> or <a href=https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html?> Series.unique() </a> <br>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html?> df.columns </a> <br>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iterrows.html?> df.iterrows() </a> <br>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.Series.to_list.html?> Series.to_list()</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac8dd01",
   "metadata": {},
   "source": [
    "### Method 1: create an empty dataframe and enter data for each row (maybe a verbose appraoch) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dac175",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create an empty dataframe having TSA as the index and dates as column headers\n",
    "daily_death_tsa = pd.DataFrame(index=tsa_county['TSA'].unique(), columns=daily_death.columns)\n",
    "daily_death_tsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26926391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, row in daily_death_tsa.iterrows():  # iterating each row of the dataframe\n",
    "    temp_list = tsa_county.loc[tsa_county['TSA'] == idx, 'County'].to_list() # Return associate county of TSA into a list\n",
    "    daily_death_tsa.loc[idx] = daily_death.loc[temp_list].sum()  # Sum the value (death) of counties \n",
    "    \n",
    "daily_death_tsa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27455066",
   "metadata": {},
   "source": [
    "### Slicing dataframes\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.at.html?> df.at </a>: Access a single value for a row/column label pair.\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html> df.loc </a>: Access a group of rows and columns by label(s).\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iat.html> df.iat </a>: Access a single value for a row/column pair by integer position.\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html> df.iloc </a>: Access a group of rows and columns by integer position(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf482ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_death_tsa.at['A', '06/01/2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef8afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_death_tsa.iat[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547432fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_death_tsa.loc['A', ['06/01/2020', '06/02/2020', '06/03/2020']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431f1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_death_tsa.iloc[0, 0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9cf210",
   "metadata": {},
   "source": [
    "---\n",
    "### *Exercise*\n",
    "1. Slice the fatality of July 4, 2020, in TSA Q, and assign the result as `df1`.\n",
    "2. Sum the fatality between December 29, 2020, and December 31, 2020, in TSA E, and assign the result as `f1`.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eddba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bff041",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Test code for the previous function. This cell should NOT give any errors when it is run.\"\"\"\n",
    "\n",
    "# Check your result here. \n",
    "assert df1 == 34\n",
    "assert f1 == 279\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782c5588",
   "metadata": {},
   "source": [
    "### Method 2: using 'Groupby' function of dataframe\n",
    "#### covered functions\n",
    "\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html> df.join() </a>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html?> df.drop() </a>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html?> df.groupby() </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23ec9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use `groupby()` function, we need to have the TSA names in the dataframe. \n",
    "daily_case_tsa = daily_case.join(tsa_county.set_index('County')) # Join tsa_county table into daily_case table based on the index (Couunty)\n",
    "daily_case_tsa = daily_case_tsa.drop(columns=['FIPS']) # Remove an uncessary column for groupby, which is FIPS code\n",
    "daily_case_tsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760f6f8d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "daily_case_tsa = daily_case_tsa.groupby(by='TSA').sum() # This function groups all the cells in the dataframe based on 'TSA' and return the sum.\n",
    "daily_case_tsa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44fe30f",
   "metadata": {},
   "source": [
    "## 3. Moving 7 days mean\n",
    "Given the fact that the census is usually collected during the weekdays, we may want to smooth the temporal fluctuation to avoid the potential bias from the days that the census took place. \n",
    "#### covered functions\n",
    "\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html?> df.plot() </a> or \n",
    "<a href=https://pandas.pydata.org/docs/reference/api/pandas.Series.plot.html> Series.plot() </a>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html?> df.copy() </a>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.fillna.html?> df.fillna() </a>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.transpose.html?> df.transpose() </a>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shape.html?> df.shape </a>\n",
    "\n",
    "import <a href=https://docs.python.org/3/library/datetime.html> datetime </a>, <a href=https://tqdm.github.io/> tqdm </a>\n",
    "* <a href=https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior> datetime.datetime.strptime() </a>\n",
    "* <a href=https://docs.python.org/3/library/datetime.html#datetime.timedelta> datetime.timedelta() </a>\n",
    "* tqdm(df.iterrows(), total=df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Too much dynamic temporal fluctuation from the raw data\n",
    "daily_death_tsa.loc['E'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a558c3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same here. \n",
    "daily_case_tsa.loc['E'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime  # a package to manipulate date and time data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bdaad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First, we create a list that has all the target dates\n",
    "from_date = '06/01/2020'\n",
    "to_date = '12/31/2020'\n",
    "\n",
    "start_date = datetime.datetime.strptime(from_date, \"%m/%d/%Y\")  # Function to change inputs from string to datetime\n",
    "end_date = datetime.datetime.strptime(to_date, \"%m/%d/%Y\")\n",
    "print(from_date, type(from_date))\n",
    "print(start_date, type(start_date))\n",
    "\n",
    "focus_dates = []\n",
    "delta = datetime.timedelta(days=1) # determine the gap between two dates as a day\n",
    "print(delta, type(delta))\n",
    "\n",
    "while start_date <= end_date:\n",
    "    focus_dates.append(start_date.strftime(\"%m/%d/%Y\"))\n",
    "    start_date += delta\n",
    "    \n",
    "print(focus_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275f9545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary that has keys as target date and values as the date that should be averaged. \n",
    "focus_date_dict = {}\n",
    "time_delta = [3, 2, 1, 0, -1, -2, -3]\n",
    "\n",
    "for idx, date in enumerate(focus_dates):\n",
    "    temp_list = []\n",
    "    for delta in time_delta:\n",
    "        temp_list.append(\n",
    "            str(\n",
    "                (datetime.datetime.strptime(focus_dates[idx], \"%m/%d/%Y\") - datetime.timedelta(days=delta)\n",
    "                ).strftime(\"%m/%d/%Y\"))\n",
    "        )\n",
    "        \n",
    "    focus_date_dict[date] = temp_list\n",
    "    \n",
    "# Manually enter the dates that would have missing values\n",
    "focus_date_dict['06/01/2020'] = ['06/01/2020', '06/02/2020', '06/03/2020', '06/04/2020']\n",
    "focus_date_dict['06/02/2020'] = ['06/01/2020', '06/02/2020', '06/03/2020', '06/04/2020', '06/05/2020']\n",
    "focus_date_dict['06/03/2020'] = ['06/01/2020', '06/02/2020', '06/03/2020', '06/04/2020', '06/05/2020', '06/06/2020']\n",
    "focus_date_dict['12/29/2020'] = ['12/26/2020', '12/27/2020', '12/28/2020', '12/29/2020', '12/30/2020', '12/31/2020']\n",
    "focus_date_dict['12/30/2020'] = ['12/27/2020', '12/28/2020', '12/29/2020', '12/30/2020', '12/31/2020']\n",
    "focus_date_dict['12/31/2020'] = ['12/28/2020', '12/29/2020', '12/30/2020', '12/31/2020']\n",
    "print(focus_date_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eef81df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # useful package to estimate computation time of loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3727a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another verbose approach (looping through rows)\n",
    "mean_death_tsa = daily_death_tsa.copy(deep=True)\n",
    "\n",
    "# Iterating through the rows and tqdm package estimates the total computational time\n",
    "for idx, row in tqdm(mean_death_tsa.iterrows(), total=mean_death_tsa.shape[0]): \n",
    "    for col in mean_death_tsa.columns: # Iterate through column here\n",
    "        mean_death_tsa.at[idx, col] = daily_death_tsa.loc[idx, focus_date_dict[col]].mean()\n",
    "\n",
    "mean_death_tsa.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe a better approach\n",
    "mean_case_tsa = daily_case_tsa.copy(deep=True)\n",
    "\n",
    "for col in mean_case_tsa.columns: # Just iterate through column (no rows iteration)\n",
    "    # Simply assigned the mean value to the entire Series\n",
    "    mean_case_tsa[col] = daily_case_tsa[focus_date_dict[col]].mean(axis=1) \n",
    "\n",
    "mean_case_tsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eebed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result of moving mean\n",
    "mean_case_tsa.loc['E'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2941b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_case_tsa.loc['E'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5eeec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compute using two dataframe that has the same structure, you can simply do the below. \n",
    "mean_ratio = mean_death_tsa / mean_case_tsa * 100\n",
    "mean_ratio = mean_ratio.fillna(0)\n",
    "mean_ratio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ratio.transpose().plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb4c1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate 7 days mean for the ICU beds availability\n",
    "mean_icu = icu_empty.copy(deep=True)\n",
    "\n",
    "for col in icu_empty.columns:\n",
    "    mean_icu[col] = icu_empty[focus_date_dict[col]].mean(axis=1)\n",
    "\n",
    "mean_icu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831568a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_icu.transpose().plot(figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1eea86",
   "metadata": {},
   "source": [
    "## 4. Correlation analysis: Examine the relationship between ICU availability and COVID-19 fatality\n",
    "\n",
    "#### covered functions\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html?> df.corr()</a>\n",
    "* <a href=https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.scatter.html?> df.plot.scatter() </a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_plot = pd.DataFrame(columns=['case_fatality_ratio', 'empty_ICU_ratio'])\n",
    "\n",
    "for idx, row in tqdm(mean_icu.iterrows(), total=mean_icu.shape[0]):\n",
    "    for col in mean_icu.columns:\n",
    "        for_plot = for_plot.append({'case_fatality_ratio': mean_ratio.loc[idx, col], 'empty_ICU_ratio': mean_icu.loc[idx,col]} ,ignore_index=True)\n",
    "\n",
    "for_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c2f532",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_plot.plot.scatter('case_fatality_ratio', 'empty_ICU_ratio', figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc08da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_plot.corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
